{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learned_index_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHeOF5-MAYxu"
      },
      "source": [
        "Author: Xinnan Shen<br>\n",
        "Date: 01-09-2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rLZxw9QAcdB"
      },
      "source": [
        "# Learned Index Model Classification Demo\n",
        "This notebook is to build some simple learned index models based on Kraska's paper. (classification model)\n",
        "\n",
        "In this model, I have used pages instead of real locations in databases in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHK6bMtAk0_"
      },
      "source": [
        "Step 1: Generate some simple dataset in csv format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuYn3rR9AQXp"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "import random\n",
        "#function: data_generation\n",
        "#usage: generate a simple dataset\n",
        "#parameters:\n",
        "#1.len_num: the size of dataset\n",
        "#2.range_min: the minimum key\n",
        "#3.range_max: the maximun key\n",
        "#output dataset: two columns (key,location)\n",
        "def data_generation(len_num,range_min,range_max):\n",
        "\tdataset=set()\n",
        "\tfor i in range(0,len_num):\n",
        "\t\tx=random.randint(range_min,range_max)\n",
        "\t\twhile x in dataset:\n",
        "\t\t\tx=random.randint(range_min,range_max)\n",
        "\t\tdataset.add(x)\n",
        "\t# for i in range(0,len(datalist)):\n",
        "\t# \ttemp=False\n",
        "\t# \tfor j in range(0,len(datalist)-i-1):\n",
        "\t# \t\tif datalist[j]>datalist[j+1]:\n",
        "\t# \t\t\tt=datalist[j]\n",
        "\t# \t\t\tdatalist[j]=datalist[j+1]\n",
        "\t# \t\t\tdatalist[j+1]=t\n",
        "\t# \t\t\ttemp=True\n",
        "\t# \tif not temp:\n",
        "\t# \t\tbreak\n",
        "\tcurrent_path=os.path.abspath(os.curdir)\n",
        "\tf=codecs.open(os.path.join(current_path,\"data.csv\"), \"w\", \"utf-8\")\n",
        "\ti=0\n",
        "\tdatalist=list(dataset)\n",
        "\tdatalist.sort()\n",
        "\tfor ele in datalist:\n",
        "\t\tf.write(str(ele)+\",\"+str(int(i/(len_num/100)))+\"\\n\")\n",
        "\t\ti=i+1\n",
        "\tf.close()\n",
        "\treturn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzMM54m-A0r_"
      },
      "source": [
        "Provide some value and generate the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcLT1jr9A1MT"
      },
      "source": [
        "minkey=0\n",
        "maxkey=2147483647\n",
        "keynum=5000000\n",
        "data_generation(keynum,minkey,maxkey)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smOkH8OIBUD-"
      },
      "source": [
        "Step 2: Split the dataset into training, development and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oe6Wz-nBWbv",
        "outputId": "a0eeb5ba-302e-4c7f-a641-dbc3e7f4a02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "from random import shuffle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "current_path=os.path.abspath(os.curdir)\n",
        "f=codecs.open(os.path.join(current_path,\"data.csv\"), \"r\", \"utf-8\")\n",
        "strlist=f.read().split(\"\\n\")\n",
        "f.close()\n",
        "list_key=[]\n",
        "list_res=[]\n",
        "for ele in strlist:\n",
        "    temp=ele.split(\",\")\n",
        "    if len(temp)!=2:\n",
        "        continue\n",
        "    list_key.append(temp[0])\n",
        "    list_res.append(temp[1])\n",
        "keys=np.array(list_key)\n",
        "res=np.array(list_res)\n",
        "trainkeys,testkeys,trainres,testres=train_test_split(keys,res,test_size=0.35)\n",
        "trainkeys,devkeys,trainres,devres=train_test_split(trainkeys,trainres,test_size=0.5)\n",
        "trainkeys=list(trainkeys)\n",
        "devkeys=list(devkeys)\n",
        "testkeys=list(testkeys)\n",
        "trainres=list(trainres)\n",
        "devres=list(devres)\n",
        "testres=list(testres)\n",
        "\n",
        "f=codecs.open(os.path.join(current_path,\"data_train.csv\"), \"w\", \"utf-8\")\n",
        "for i in range(0,len(trainkeys)):\n",
        "    f.write(str(trainkeys[i])+\",\"+str(trainres[i])+\"\\n\")\n",
        "f.close()\n",
        "f=codecs.open(os.path.join(current_path,\"data_dev.csv\"), \"w\", \"utf-8\")\n",
        "for i in range(0,len(devkeys)):\n",
        "    f.write(str(devkeys[i])+\",\"+str(devres[i])+\"\\n\")\n",
        "f.close()\n",
        "f=codecs.open(os.path.join(current_path,\"data_test.csv\"), \"w\", \"utf-8\")\n",
        "for i in range(0,len(testkeys)):\n",
        "    f.write(str(testkeys[i])+\",\"+str(testres[i])+\"\\n\")\n",
        "f.close()\n",
        "print(\"training data size:\",len(trainkeys))\n",
        "print(\"development data size:\",len(devkeys))\n",
        "print(\"testing data size:\",len(testkeys))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-4ea989f32d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstrlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1MPUWbdBlmc",
        "outputId": "acc2ae25-6653-49b9-b3ae-881232de3c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import codecs\n",
        "import os\n",
        "minkey=1000\n",
        "maxkey=9999\n",
        "keynum=3000\n",
        "current_path=os.path.abspath(os.curdir)\n",
        "f=codecs.open(os.path.join(current_path,\"data_train.csv\"), \"r\", \"utf-8\")\n",
        "strlist=f.read().split(\"\\n\")\n",
        "f.close()\n",
        "trainkeys=[]\n",
        "trainres=[]\n",
        "for ele in strlist:\n",
        "    temp=ele.split(\",\")\n",
        "    if len(temp)!=2:\n",
        "        continue\n",
        "    trainkeys.append(int(temp[0]))\n",
        "    trainres.append(int(temp[1]))\n",
        "f=codecs.open(os.path.join(current_path,\"data_dev.csv\"), \"r\", \"utf-8\")\n",
        "strlist=f.read().split(\"\\n\")\n",
        "f.close()\n",
        "devkeys=[]\n",
        "devres=[]\n",
        "for ele in strlist:\n",
        "    temp=ele.split(\",\")\n",
        "    if len(temp)!=2:\n",
        "        continue\n",
        "    devkeys.append(int(temp[0]))\n",
        "    devres.append(int(temp[1]))\n",
        "f=codecs.open(os.path.join(current_path,\"data_test.csv\"), \"r\", \"utf-8\")\n",
        "strlist=f.read().split(\"\\n\")\n",
        "f.close()\n",
        "testkeys=[]\n",
        "testres=[]\n",
        "for ele in strlist:\n",
        "    temp=ele.split(\",\")\n",
        "    if len(temp)!=2:\n",
        "        continue\n",
        "    testkeys.append(int(temp[0]))\n",
        "    testres.append(int(temp[1]))\n",
        "print(\"training data size:\",len(trainkeys))\n",
        "print(\"development data size:\",len(devkeys))\n",
        "print(\"testing data size:\",len(testkeys))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 1625000\n",
            "development data size: 1625000\n",
            "testing data size: 1750000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrDzx8jnK2YT"
      },
      "source": [
        "import numpy as np\n",
        "X_train=np.array(trainkeys).reshape(-1,1)\n",
        "Y_train=np.array(trainres).reshape(-1,1)\n",
        "X_dev=np.array(devkeys).reshape(-1,1)\n",
        "Y_dev=np.array(devres).reshape(-1,1)\n",
        "X_test=np.array(testkeys).reshape(-1,1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUb22SHBpLD"
      },
      "source": [
        "Step 4: Build a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C24Ti5yfEhp9"
      },
      "source": [
        "- KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yOO0tx9BrWQ",
        "outputId": "3b07a77f-f206-427b-bf62-150ca70d117e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "t1=time.time()\n",
        "neigh = KNeighborsClassifier(n_neighbors=9)\n",
        "neigh.fit(X_train,Y_train)\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "devpre=neigh.predict(X_dev)#.reshape(1,-1).tolist()[0]\n",
        "print(classification_report(Y_dev,devpre))\n",
        "print(\"time interval for building model:\"+str(time_interval*1000)+\" ms\")\n",
        "t1=time.time()\n",
        "testpre=neigh.predict(X_test)#.reshape(1,-1).tolist()[0]\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "print(\"time interval for indexing data:\"+str(time_interval*1000)+\" ms\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     16251\n",
            "           1       1.00      1.00      1.00     16224\n",
            "           2       1.00      1.00      1.00     16227\n",
            "           3       1.00      1.00      1.00     16269\n",
            "           4       1.00      1.00      1.00     16354\n",
            "           5       1.00      1.00      1.00     16234\n",
            "           6       1.00      1.00      1.00     16364\n",
            "           7       1.00      1.00      1.00     16104\n",
            "           8       1.00      1.00      1.00     16316\n",
            "           9       1.00      1.00      1.00     16212\n",
            "          10       1.00      1.00      1.00     16330\n",
            "          11       1.00      1.00      1.00     16042\n",
            "          12       1.00      1.00      1.00     16258\n",
            "          13       1.00      1.00      1.00     16194\n",
            "          14       1.00      1.00      1.00     16213\n",
            "          15       1.00      1.00      1.00     16316\n",
            "          16       1.00      1.00      1.00     16430\n",
            "          17       1.00      1.00      1.00     16125\n",
            "          18       1.00      1.00      1.00     16355\n",
            "          19       1.00      1.00      1.00     16343\n",
            "          20       1.00      1.00      1.00     16240\n",
            "          21       1.00      1.00      1.00     16267\n",
            "          22       1.00      1.00      1.00     16094\n",
            "          23       1.00      1.00      1.00     16372\n",
            "          24       1.00      1.00      1.00     16253\n",
            "          25       1.00      1.00      1.00     16184\n",
            "          26       1.00      1.00      1.00     16270\n",
            "          27       1.00      1.00      1.00     16133\n",
            "          28       1.00      1.00      1.00     16457\n",
            "          29       1.00      1.00      1.00     16522\n",
            "          30       1.00      1.00      1.00     15895\n",
            "          31       1.00      1.00      1.00     16406\n",
            "          32       1.00      1.00      1.00     16129\n",
            "          33       1.00      1.00      1.00     16114\n",
            "          34       1.00      1.00      1.00     16338\n",
            "          35       1.00      1.00      1.00     16177\n",
            "          36       1.00      1.00      1.00     16309\n",
            "          37       1.00      1.00      1.00     16151\n",
            "          38       1.00      1.00      1.00     16121\n",
            "          39       1.00      1.00      1.00     16384\n",
            "          40       1.00      1.00      1.00     16278\n",
            "          41       1.00      1.00      1.00     16334\n",
            "          42       1.00      1.00      1.00     16025\n",
            "          43       1.00      1.00      1.00     16289\n",
            "          44       1.00      1.00      1.00     16331\n",
            "          45       1.00      1.00      1.00     16130\n",
            "          46       1.00      1.00      1.00     16194\n",
            "          47       1.00      1.00      1.00     16254\n",
            "          48       1.00      1.00      1.00     16288\n",
            "          49       1.00      1.00      1.00     16198\n",
            "          50       1.00      1.00      1.00     16287\n",
            "          51       1.00      1.00      1.00     16135\n",
            "          52       1.00      1.00      1.00     16229\n",
            "          53       1.00      1.00      1.00     16326\n",
            "          54       1.00      1.00      1.00     16409\n",
            "          55       1.00      1.00      1.00     16354\n",
            "          56       1.00      1.00      1.00     16264\n",
            "          57       1.00      1.00      1.00     16173\n",
            "          58       1.00      1.00      1.00     16391\n",
            "          59       1.00      1.00      1.00     16213\n",
            "          60       1.00      1.00      1.00     16145\n",
            "          61       1.00      1.00      1.00     16084\n",
            "          62       1.00      1.00      1.00     16307\n",
            "          63       1.00      1.00      1.00     16246\n",
            "          64       1.00      1.00      1.00     16271\n",
            "          65       1.00      1.00      1.00     16378\n",
            "          66       1.00      1.00      1.00     16287\n",
            "          67       1.00      1.00      1.00     16151\n",
            "          68       1.00      1.00      1.00     16107\n",
            "          69       1.00      1.00      1.00     16259\n",
            "          70       1.00      1.00      1.00     16281\n",
            "          71       1.00      1.00      1.00     16278\n",
            "          72       1.00      1.00      1.00     16333\n",
            "          73       1.00      1.00      1.00     16149\n",
            "          74       1.00      1.00      1.00     16118\n",
            "          75       1.00      1.00      1.00     16306\n",
            "          76       1.00      1.00      1.00     16285\n",
            "          77       1.00      1.00      1.00     16211\n",
            "          78       1.00      1.00      1.00     16237\n",
            "          79       1.00      1.00      1.00     16140\n",
            "          80       1.00      1.00      1.00     16171\n",
            "          81       1.00      1.00      1.00     16350\n",
            "          82       1.00      1.00      1.00     16329\n",
            "          83       1.00      1.00      1.00     16007\n",
            "          84       1.00      1.00      1.00     16322\n",
            "          85       1.00      1.00      1.00     16342\n",
            "          86       1.00      1.00      1.00     16393\n",
            "          87       1.00      1.00      1.00     16317\n",
            "          88       1.00      1.00      1.00     16310\n",
            "          89       1.00      1.00      1.00     16266\n",
            "          90       1.00      1.00      1.00     16272\n",
            "          91       1.00      1.00      1.00     16136\n",
            "          92       1.00      1.00      1.00     16348\n",
            "          93       1.00      1.00      1.00     16077\n",
            "          94       1.00      1.00      1.00     16147\n",
            "          95       1.00      1.00      1.00     16204\n",
            "          96       1.00      1.00      1.00     16424\n",
            "          97       1.00      1.00      1.00     16259\n",
            "          98       1.00      1.00      1.00     16383\n",
            "          99       1.00      1.00      1.00     16291\n",
            "\n",
            "    accuracy                           1.00   1625000\n",
            "   macro avg       1.00      1.00      1.00   1625000\n",
            "weighted avg       1.00      1.00      1.00   1625000\n",
            "\n",
            "time interval for building model:13714.377164840698 ms\n",
            "time interval for indexing data:40775.90489387512 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShKGx7rbEknq"
      },
      "source": [
        "- Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7crWFVhEJ-X",
        "outputId": "aa656468-6f26-4a11-82cc-a13eaed5f507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "t1=time.time()\n",
        "NB = GaussianNB()\n",
        "NB.fit(X_train,Y_train)\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "devpre=NB.predict(X_dev)#.reshape(1,-1).tolist()[0]\n",
        "print(classification_report(Y_dev,devpre))\n",
        "print(\"time interval for building model:\"+str(time_interval*1000)+\" ms\")\n",
        "t1=time.time()\n",
        "testpre=NB.predict(X_test)#.reshape(1,-1).tolist()[0]\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "print(\"time interval for indexing data:\"+str(time_interval*1000)+\" ms\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     16251\n",
            "           1       1.00      1.00      1.00     16224\n",
            "           2       1.00      1.00      1.00     16227\n",
            "           3       1.00      1.00      1.00     16269\n",
            "           4       1.00      1.00      1.00     16354\n",
            "           5       1.00      1.00      1.00     16234\n",
            "           6       1.00      1.00      1.00     16364\n",
            "           7       1.00      1.00      1.00     16104\n",
            "           8       1.00      1.00      1.00     16316\n",
            "           9       1.00      1.00      1.00     16212\n",
            "          10       1.00      1.00      1.00     16330\n",
            "          11       1.00      1.00      1.00     16042\n",
            "          12       0.99      1.00      1.00     16258\n",
            "          13       1.00      0.99      1.00     16194\n",
            "          14       1.00      1.00      1.00     16213\n",
            "          15       1.00      1.00      1.00     16316\n",
            "          16       1.00      1.00      1.00     16430\n",
            "          17       1.00      1.00      1.00     16125\n",
            "          18       1.00      1.00      1.00     16355\n",
            "          19       1.00      1.00      1.00     16343\n",
            "          20       1.00      1.00      1.00     16240\n",
            "          21       1.00      1.00      1.00     16267\n",
            "          22       1.00      1.00      1.00     16094\n",
            "          23       1.00      1.00      1.00     16372\n",
            "          24       1.00      1.00      1.00     16253\n",
            "          25       1.00      1.00      1.00     16184\n",
            "          26       1.00      1.00      1.00     16270\n",
            "          27       1.00      1.00      1.00     16133\n",
            "          28       1.00      1.00      1.00     16457\n",
            "          29       1.00      1.00      1.00     16522\n",
            "          30       1.00      1.00      1.00     15895\n",
            "          31       1.00      1.00      1.00     16406\n",
            "          32       1.00      1.00      1.00     16129\n",
            "          33       1.00      1.00      1.00     16114\n",
            "          34       1.00      1.00      1.00     16338\n",
            "          35       1.00      1.00      1.00     16177\n",
            "          36       1.00      1.00      1.00     16309\n",
            "          37       1.00      1.00      1.00     16151\n",
            "          38       1.00      1.00      1.00     16121\n",
            "          39       1.00      1.00      1.00     16384\n",
            "          40       1.00      1.00      1.00     16278\n",
            "          41       1.00      1.00      1.00     16334\n",
            "          42       1.00      1.00      1.00     16025\n",
            "          43       1.00      1.00      1.00     16289\n",
            "          44       1.00      1.00      1.00     16331\n",
            "          45       1.00      1.00      1.00     16130\n",
            "          46       1.00      1.00      1.00     16194\n",
            "          47       1.00      1.00      1.00     16254\n",
            "          48       1.00      1.00      1.00     16288\n",
            "          49       1.00      1.00      1.00     16198\n",
            "          50       1.00      1.00      1.00     16287\n",
            "          51       1.00      1.00      1.00     16135\n",
            "          52       1.00      1.00      1.00     16229\n",
            "          53       1.00      1.00      1.00     16326\n",
            "          54       1.00      1.00      1.00     16409\n",
            "          55       1.00      1.00      1.00     16354\n",
            "          56       1.00      1.00      1.00     16264\n",
            "          57       1.00      1.00      1.00     16173\n",
            "          58       1.00      1.00      1.00     16391\n",
            "          59       1.00      1.00      1.00     16213\n",
            "          60       1.00      1.00      1.00     16145\n",
            "          61       1.00      1.00      1.00     16084\n",
            "          62       1.00      1.00      1.00     16307\n",
            "          63       1.00      1.00      1.00     16246\n",
            "          64       1.00      1.00      1.00     16271\n",
            "          65       1.00      1.00      1.00     16378\n",
            "          66       1.00      1.00      1.00     16287\n",
            "          67       1.00      1.00      1.00     16151\n",
            "          68       1.00      1.00      1.00     16107\n",
            "          69       1.00      0.99      1.00     16259\n",
            "          70       1.00      1.00      1.00     16281\n",
            "          71       1.00      1.00      1.00     16278\n",
            "          72       1.00      1.00      1.00     16333\n",
            "          73       1.00      1.00      1.00     16149\n",
            "          74       1.00      1.00      1.00     16118\n",
            "          75       1.00      1.00      1.00     16306\n",
            "          76       1.00      1.00      1.00     16285\n",
            "          77       1.00      1.00      1.00     16211\n",
            "          78       1.00      1.00      1.00     16237\n",
            "          79       1.00      1.00      1.00     16140\n",
            "          80       1.00      1.00      1.00     16171\n",
            "          81       1.00      1.00      1.00     16350\n",
            "          82       1.00      1.00      1.00     16329\n",
            "          83       1.00      1.00      1.00     16007\n",
            "          84       1.00      1.00      1.00     16322\n",
            "          85       1.00      1.00      1.00     16342\n",
            "          86       1.00      1.00      1.00     16393\n",
            "          87       1.00      1.00      1.00     16317\n",
            "          88       0.99      1.00      1.00     16310\n",
            "          89       1.00      1.00      1.00     16266\n",
            "          90       1.00      1.00      1.00     16272\n",
            "          91       1.00      1.00      1.00     16136\n",
            "          92       1.00      1.00      1.00     16348\n",
            "          93       1.00      1.00      1.00     16077\n",
            "          94       1.00      1.00      1.00     16147\n",
            "          95       1.00      1.00      1.00     16204\n",
            "          96       1.00      1.00      1.00     16424\n",
            "          97       1.00      1.00      1.00     16259\n",
            "          98       1.00      1.00      1.00     16383\n",
            "          99       1.00      1.00      1.00     16291\n",
            "\n",
            "    accuracy                           1.00   1625000\n",
            "   macro avg       1.00      1.00      1.00   1625000\n",
            "weighted avg       1.00      1.00      1.00   1625000\n",
            "\n",
            "time interval for building model:610.6061935424805 ms\n",
            "time interval for indexing data:2609.384298324585 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVW-ZWqwFQCv"
      },
      "source": [
        "- Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0FgFU3KFRXd",
        "outputId": "8a98ef8a-4a48-4538-fe8b-429969d52290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import tree\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "t1=time.time()\n",
        "tree = tree.DecisionTreeClassifier()\n",
        "tree.fit(X_train,Y_train)\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "devpre=tree.predict(X_dev)#.reshape(1,-1).tolist()[0]\n",
        "print(classification_report(Y_dev,devpre))\n",
        "print(\"time interval for building model:\"+str(time_interval*1000)+\" ms\")\n",
        "t1=time.time()\n",
        "testpre=tree.predict(X_test)#.reshape(1,-1).tolist()[0]\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "print(\"time interval for indexing data:\"+str(time_interval*1000)+\" ms\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     16251\n",
            "           1       1.00      1.00      1.00     16224\n",
            "           2       1.00      1.00      1.00     16227\n",
            "           3       1.00      1.00      1.00     16269\n",
            "           4       1.00      1.00      1.00     16354\n",
            "           5       1.00      1.00      1.00     16234\n",
            "           6       1.00      1.00      1.00     16364\n",
            "           7       1.00      1.00      1.00     16104\n",
            "           8       1.00      1.00      1.00     16316\n",
            "           9       1.00      1.00      1.00     16212\n",
            "          10       1.00      1.00      1.00     16330\n",
            "          11       1.00      1.00      1.00     16042\n",
            "          12       1.00      1.00      1.00     16258\n",
            "          13       1.00      1.00      1.00     16194\n",
            "          14       1.00      1.00      1.00     16213\n",
            "          15       1.00      1.00      1.00     16316\n",
            "          16       1.00      1.00      1.00     16430\n",
            "          17       1.00      1.00      1.00     16125\n",
            "          18       1.00      1.00      1.00     16355\n",
            "          19       1.00      1.00      1.00     16343\n",
            "          20       1.00      1.00      1.00     16240\n",
            "          21       1.00      1.00      1.00     16267\n",
            "          22       1.00      1.00      1.00     16094\n",
            "          23       1.00      1.00      1.00     16372\n",
            "          24       1.00      1.00      1.00     16253\n",
            "          25       1.00      1.00      1.00     16184\n",
            "          26       1.00      1.00      1.00     16270\n",
            "          27       1.00      1.00      1.00     16133\n",
            "          28       1.00      1.00      1.00     16457\n",
            "          29       1.00      1.00      1.00     16522\n",
            "          30       1.00      1.00      1.00     15895\n",
            "          31       1.00      1.00      1.00     16406\n",
            "          32       1.00      1.00      1.00     16129\n",
            "          33       1.00      1.00      1.00     16114\n",
            "          34       1.00      1.00      1.00     16338\n",
            "          35       1.00      1.00      1.00     16177\n",
            "          36       1.00      1.00      1.00     16309\n",
            "          37       1.00      1.00      1.00     16151\n",
            "          38       1.00      1.00      1.00     16121\n",
            "          39       1.00      1.00      1.00     16384\n",
            "          40       1.00      1.00      1.00     16278\n",
            "          41       1.00      1.00      1.00     16334\n",
            "          42       1.00      1.00      1.00     16025\n",
            "          43       1.00      1.00      1.00     16289\n",
            "          44       1.00      1.00      1.00     16331\n",
            "          45       1.00      1.00      1.00     16130\n",
            "          46       1.00      1.00      1.00     16194\n",
            "          47       1.00      1.00      1.00     16254\n",
            "          48       1.00      1.00      1.00     16288\n",
            "          49       1.00      1.00      1.00     16198\n",
            "          50       1.00      1.00      1.00     16287\n",
            "          51       1.00      1.00      1.00     16135\n",
            "          52       1.00      1.00      1.00     16229\n",
            "          53       1.00      1.00      1.00     16326\n",
            "          54       1.00      1.00      1.00     16409\n",
            "          55       1.00      1.00      1.00     16354\n",
            "          56       1.00      1.00      1.00     16264\n",
            "          57       1.00      1.00      1.00     16173\n",
            "          58       1.00      1.00      1.00     16391\n",
            "          59       1.00      1.00      1.00     16213\n",
            "          60       1.00      1.00      1.00     16145\n",
            "          61       1.00      1.00      1.00     16084\n",
            "          62       1.00      1.00      1.00     16307\n",
            "          63       1.00      1.00      1.00     16246\n",
            "          64       1.00      1.00      1.00     16271\n",
            "          65       1.00      1.00      1.00     16378\n",
            "          66       1.00      1.00      1.00     16287\n",
            "          67       1.00      1.00      1.00     16151\n",
            "          68       1.00      1.00      1.00     16107\n",
            "          69       1.00      1.00      1.00     16259\n",
            "          70       1.00      1.00      1.00     16281\n",
            "          71       1.00      1.00      1.00     16278\n",
            "          72       1.00      1.00      1.00     16333\n",
            "          73       1.00      1.00      1.00     16149\n",
            "          74       1.00      1.00      1.00     16118\n",
            "          75       1.00      1.00      1.00     16306\n",
            "          76       1.00      1.00      1.00     16285\n",
            "          77       1.00      1.00      1.00     16211\n",
            "          78       1.00      1.00      1.00     16237\n",
            "          79       1.00      1.00      1.00     16140\n",
            "          80       1.00      1.00      1.00     16171\n",
            "          81       1.00      1.00      1.00     16350\n",
            "          82       1.00      1.00      1.00     16329\n",
            "          83       1.00      1.00      1.00     16007\n",
            "          84       1.00      1.00      1.00     16322\n",
            "          85       1.00      1.00      1.00     16342\n",
            "          86       1.00      1.00      1.00     16393\n",
            "          87       1.00      1.00      1.00     16317\n",
            "          88       1.00      1.00      1.00     16310\n",
            "          89       1.00      1.00      1.00     16266\n",
            "          90       1.00      1.00      1.00     16272\n",
            "          91       1.00      1.00      1.00     16136\n",
            "          92       1.00      1.00      1.00     16348\n",
            "          93       1.00      1.00      1.00     16077\n",
            "          94       1.00      1.00      1.00     16147\n",
            "          95       1.00      1.00      1.00     16204\n",
            "          96       1.00      1.00      1.00     16424\n",
            "          97       1.00      1.00      1.00     16259\n",
            "          98       1.00      1.00      1.00     16383\n",
            "          99       1.00      1.00      1.00     16291\n",
            "\n",
            "    accuracy                           1.00   1625000\n",
            "   macro avg       1.00      1.00      1.00   1625000\n",
            "weighted avg       1.00      1.00      1.00   1625000\n",
            "\n",
            "time interval for building model:9478.073358535767 ms\n",
            "time interval for indexing data:532.5093269348145 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HestQfJKF_Qb"
      },
      "source": [
        "- Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSITa6x6GAqN",
        "outputId": "f197d2a9-e8c5-4ed3-c5f0-8cc2931c9a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "t1=time.time()\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X_train,Y_train)\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "devpre=forest.predict(X_dev)#.reshape(1,-1).tolist()[0]\n",
        "print(classification_report(Y_dev,devpre))\n",
        "print(\"time interval for building model:\"+str(time_interval*1000)+\" ms\")\n",
        "t1=time.time()\n",
        "testpre=forest.predict(X_test)#.reshape(1,-1).tolist()[0]\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "print(\"time interval for indexing data:\"+str(time_interval*1000)+\" ms\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     16251\n",
            "           1       1.00      1.00      1.00     16224\n",
            "           2       1.00      1.00      1.00     16227\n",
            "           3       1.00      1.00      1.00     16269\n",
            "           4       1.00      1.00      1.00     16354\n",
            "           5       1.00      1.00      1.00     16234\n",
            "           6       1.00      1.00      1.00     16364\n",
            "           7       1.00      1.00      1.00     16104\n",
            "           8       1.00      1.00      1.00     16316\n",
            "           9       1.00      1.00      1.00     16212\n",
            "          10       1.00      1.00      1.00     16330\n",
            "          11       1.00      1.00      1.00     16042\n",
            "          12       1.00      1.00      1.00     16258\n",
            "          13       1.00      1.00      1.00     16194\n",
            "          14       1.00      1.00      1.00     16213\n",
            "          15       1.00      1.00      1.00     16316\n",
            "          16       1.00      1.00      1.00     16430\n",
            "          17       1.00      1.00      1.00     16125\n",
            "          18       1.00      1.00      1.00     16355\n",
            "          19       1.00      1.00      1.00     16343\n",
            "          20       1.00      1.00      1.00     16240\n",
            "          21       1.00      1.00      1.00     16267\n",
            "          22       1.00      1.00      1.00     16094\n",
            "          23       1.00      1.00      1.00     16372\n",
            "          24       1.00      1.00      1.00     16253\n",
            "          25       1.00      1.00      1.00     16184\n",
            "          26       1.00      1.00      1.00     16270\n",
            "          27       1.00      1.00      1.00     16133\n",
            "          28       1.00      1.00      1.00     16457\n",
            "          29       1.00      1.00      1.00     16522\n",
            "          30       1.00      1.00      1.00     15895\n",
            "          31       1.00      1.00      1.00     16406\n",
            "          32       1.00      1.00      1.00     16129\n",
            "          33       1.00      1.00      1.00     16114\n",
            "          34       1.00      1.00      1.00     16338\n",
            "          35       1.00      1.00      1.00     16177\n",
            "          36       1.00      1.00      1.00     16309\n",
            "          37       1.00      1.00      1.00     16151\n",
            "          38       1.00      1.00      1.00     16121\n",
            "          39       1.00      1.00      1.00     16384\n",
            "          40       1.00      1.00      1.00     16278\n",
            "          41       1.00      1.00      1.00     16334\n",
            "          42       1.00      1.00      1.00     16025\n",
            "          43       1.00      1.00      1.00     16289\n",
            "          44       1.00      1.00      1.00     16331\n",
            "          45       1.00      1.00      1.00     16130\n",
            "          46       1.00      1.00      1.00     16194\n",
            "          47       1.00      1.00      1.00     16254\n",
            "          48       1.00      1.00      1.00     16288\n",
            "          49       1.00      1.00      1.00     16198\n",
            "          50       1.00      1.00      1.00     16287\n",
            "          51       1.00      1.00      1.00     16135\n",
            "          52       1.00      1.00      1.00     16229\n",
            "          53       1.00      1.00      1.00     16326\n",
            "          54       1.00      1.00      1.00     16409\n",
            "          55       1.00      1.00      1.00     16354\n",
            "          56       1.00      1.00      1.00     16264\n",
            "          57       1.00      1.00      1.00     16173\n",
            "          58       1.00      1.00      1.00     16391\n",
            "          59       1.00      1.00      1.00     16213\n",
            "          60       1.00      1.00      1.00     16145\n",
            "          61       1.00      1.00      1.00     16084\n",
            "          62       1.00      1.00      1.00     16307\n",
            "          63       1.00      1.00      1.00     16246\n",
            "          64       1.00      1.00      1.00     16271\n",
            "          65       1.00      1.00      1.00     16378\n",
            "          66       1.00      1.00      1.00     16287\n",
            "          67       1.00      1.00      1.00     16151\n",
            "          68       1.00      1.00      1.00     16107\n",
            "          69       1.00      1.00      1.00     16259\n",
            "          70       1.00      1.00      1.00     16281\n",
            "          71       1.00      1.00      1.00     16278\n",
            "          72       1.00      1.00      1.00     16333\n",
            "          73       1.00      1.00      1.00     16149\n",
            "          74       1.00      1.00      1.00     16118\n",
            "          75       1.00      1.00      1.00     16306\n",
            "          76       1.00      1.00      1.00     16285\n",
            "          77       1.00      1.00      1.00     16211\n",
            "          78       1.00      1.00      1.00     16237\n",
            "          79       1.00      1.00      1.00     16140\n",
            "          80       1.00      1.00      1.00     16171\n",
            "          81       1.00      1.00      1.00     16350\n",
            "          82       1.00      1.00      1.00     16329\n",
            "          83       1.00      1.00      1.00     16007\n",
            "          84       1.00      1.00      1.00     16322\n",
            "          85       1.00      1.00      1.00     16342\n",
            "          86       1.00      1.00      1.00     16393\n",
            "          87       1.00      1.00      1.00     16317\n",
            "          88       1.00      1.00      1.00     16310\n",
            "          89       1.00      1.00      1.00     16266\n",
            "          90       1.00      1.00      1.00     16272\n",
            "          91       1.00      1.00      1.00     16136\n",
            "          92       1.00      1.00      1.00     16348\n",
            "          93       1.00      1.00      1.00     16077\n",
            "          94       1.00      1.00      1.00     16147\n",
            "          95       1.00      1.00      1.00     16204\n",
            "          96       1.00      1.00      1.00     16424\n",
            "          97       1.00      1.00      1.00     16259\n",
            "          98       1.00      1.00      1.00     16383\n",
            "          99       1.00      1.00      1.00     16291\n",
            "\n",
            "    accuracy                           1.00   1625000\n",
            "   macro avg       1.00      1.00      1.00   1625000\n",
            "weighted avg       1.00      1.00      1.00   1625000\n",
            "\n",
            "time interval for building model:651162.5061035156 ms\n",
            "time interval for indexing data:88819.03433799744 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD_Gken5quZ7"
      },
      "source": [
        "- DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiEkxrJitVtl"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical   \n",
        "Y_train_NN = to_categorical(Y_train, num_classes=100)\n",
        "Y_dev_NN = to_categorical(Y_dev, num_classes=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe03nSf9quDp",
        "outputId": "6a89de86-1829-4966-f599-15b61e5b24fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api\n",
        "from keras import models as md\n",
        "from keras import layers as lr\n",
        "import numpy as np\n",
        "import time\n",
        "t1=time.time()\n",
        "model = md.Sequential()\n",
        "model.add(lr.Dense(128,activation=\"relu\"))\n",
        "model.add(lr.Dense(128,activation=\"relu\"))\n",
        "# model.add(lr.Dense(32,activation=\"relu\"))\n",
        "model.add(lr.Dropout(0.2))\n",
        "model.add(lr.Dense(100,activation=\"softmax\"))\n",
        "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])#compile the model\n",
        "model.fit(X_train, Y_train_NN, epochs=16, batch_size=32)#fit the model\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "loss, acc = model.evaluate(X_dev, Y_dev_NN)\n",
        "print(loss,acc)\n",
        "print(\"time interval for building model:\"+str(time_interval*1000)+\" ms\")\n",
        "t1=time.time()\n",
        "model.predict(X_test)\n",
        "t2=time.time()\n",
        "time_interval=t2-t1\n",
        "print(\"time interval for indexing data:\"+str(time_interval*1000)+\" ms\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 1401627.1250 - accuracy: 0.0097\n",
            "Epoch 2/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6055 - accuracy: 0.0096\n",
            "Epoch 3/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6055 - accuracy: 0.0101\n",
            "Epoch 4/16\n",
            "2032/2032 [==============================] - 5s 3ms/step - loss: 4.6054 - accuracy: 0.0101\n",
            "Epoch 5/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6055 - accuracy: 0.0095\n",
            "Epoch 6/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0100\n",
            "Epoch 7/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0096\n",
            "Epoch 8/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0097\n",
            "Epoch 9/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0095\n",
            "Epoch 10/16\n",
            "2032/2032 [==============================] - 5s 3ms/step - loss: 4.6054 - accuracy: 0.0099\n",
            "Epoch 11/16\n",
            "2032/2032 [==============================] - 5s 3ms/step - loss: 4.6055 - accuracy: 0.0093\n",
            "Epoch 12/16\n",
            "2032/2032 [==============================] - 5s 3ms/step - loss: 4.6054 - accuracy: 0.0101\n",
            "Epoch 13/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0097\n",
            "Epoch 14/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6055 - accuracy: 0.0096\n",
            "Epoch 15/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6055 - accuracy: 0.0095\n",
            "Epoch 16/16\n",
            "2032/2032 [==============================] - 5s 2ms/step - loss: 4.6054 - accuracy: 0.0101\n",
            "2032/2032 [==============================] - 4s 2ms/step - loss: 4.6062 - accuracy: 0.0095\n",
            "4.606197834014893 0.00950769241899252\n",
            "time interval for building model:81205.90424537659 ms\n",
            "time interval for indexing data:1416.9352054595947 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjTm0GtLvGLo",
        "outputId": "3493c6d7-40b4-4774-b176-428be99082e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_test=np.argmax(model.predict(X_test), axis=-1)\n",
        "print(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[70 70 70 ... 70 70 70]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}