# -*- coding: utf-8 -*-
"""learned index demo2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nl7yNzCjeVFVSjCxdNiDZl3hIQoWPzzY

Author: Xinnan Shen<br>
Date: 01-09-2020

# Learned Index Model Classification Demo
This notebook is to build some simple learned index models based on Kraska's paper. (classification model)

In this model, I have used pages instead of real locations in databases in the dataset.

Step 1: Generate some simple dataset in csv format
"""

import os
import codecs
import random
#function: data_generation
#usage: generate a simple dataset
#parameters:
#1.len_num: the size of dataset
#2.range_min: the minimum key
#3.range_max: the maximun key
#output dataset: two columns (key,location)
def data_generation(len_num,range_min,range_max):
	dataset=set()
	for i in range(0,len_num):
		x=random.randint(range_min,range_max)
		while x in dataset:
			x=random.randint(range_min,range_max)
		dataset.add(x)
	# for i in range(0,len(datalist)):
	# 	temp=False
	# 	for j in range(0,len(datalist)-i-1):
	# 		if datalist[j]>datalist[j+1]:
	# 			t=datalist[j]
	# 			datalist[j]=datalist[j+1]
	# 			datalist[j+1]=t
	# 			temp=True
	# 	if not temp:
	# 		break
	current_path=os.path.abspath(os.curdir)
	f=codecs.open(os.path.join(current_path,"data.csv"), "w", "utf-8")
	i=0
	datalist=list(dataset)
	datalist.sort()
	for ele in datalist:
		f.write(str(ele)+","+str(int(i/(len_num/100)))+"\n")
		i=i+1
	f.close()
	return

"""Provide some value and generate the dataset"""

minkey=0
maxkey=2147483647
keynum=5000000
data_generation(keynum,minkey,maxkey)

"""Step 2: Split the dataset into training, development and testing dataset"""

from random import shuffle
import numpy as np
from sklearn.model_selection import train_test_split
current_path=os.path.abspath(os.curdir)
f=codecs.open(os.path.join(current_path,"data.csv"), "r", "utf-8")
strlist=f.read().split("\n")
f.close()
list_key=[]
list_res=[]
for ele in strlist:
    temp=ele.split(",")
    if len(temp)!=2:
        continue
    list_key.append(temp[0])
    list_res.append(temp[1])
keys=np.array(list_key)
res=np.array(list_res)
trainkeys,testkeys,trainres,testres=train_test_split(keys,res,test_size=0.35)
trainkeys,devkeys,trainres,devres=train_test_split(trainkeys,trainres,test_size=0.5)
trainkeys=list(trainkeys)
devkeys=list(devkeys)
testkeys=list(testkeys)
trainres=list(trainres)
devres=list(devres)
testres=list(testres)

f=codecs.open(os.path.join(current_path,"data_train.csv"), "w", "utf-8")
for i in range(0,len(trainkeys)):
    f.write(str(trainkeys[i])+","+str(trainres[i])+"\n")
f.close()
f=codecs.open(os.path.join(current_path,"data_dev.csv"), "w", "utf-8")
for i in range(0,len(devkeys)):
    f.write(str(devkeys[i])+","+str(devres[i])+"\n")
f.close()
f=codecs.open(os.path.join(current_path,"data_test.csv"), "w", "utf-8")
for i in range(0,len(testkeys)):
    f.write(str(testkeys[i])+","+str(testres[i])+"\n")
f.close()
print("training data size:",len(trainkeys))
print("development data size:",len(devkeys))
print("testing data size:",len(testkeys))

import codecs
import os
minkey=1000
maxkey=9999
keynum=3000
current_path=os.path.abspath(os.curdir)
f=codecs.open(os.path.join(current_path,"data_train.csv"), "r", "utf-8")
strlist=f.read().split("\n")
f.close()
trainkeys=[]
trainres=[]
for ele in strlist:
    temp=ele.split(",")
    if len(temp)!=2:
        continue
    trainkeys.append(int(temp[0]))
    trainres.append(int(temp[1]))
f=codecs.open(os.path.join(current_path,"data_dev.csv"), "r", "utf-8")
strlist=f.read().split("\n")
f.close()
devkeys=[]
devres=[]
for ele in strlist:
    temp=ele.split(",")
    if len(temp)!=2:
        continue
    devkeys.append(int(temp[0]))
    devres.append(int(temp[1]))
f=codecs.open(os.path.join(current_path,"data_test.csv"), "r", "utf-8")
strlist=f.read().split("\n")
f.close()
testkeys=[]
testres=[]
for ele in strlist:
    temp=ele.split(",")
    if len(temp)!=2:
        continue
    testkeys.append(int(temp[0]))
    testres.append(int(temp[1]))
print("training data size:",len(trainkeys))
print("development data size:",len(devkeys))
print("testing data size:",len(testkeys))

import numpy as np
X_train=np.array(trainkeys).reshape(-1,1)
Y_train=np.array(trainres).reshape(-1,1)
X_dev=np.array(devkeys).reshape(-1,1)
Y_dev=np.array(devres).reshape(-1,1)
X_test=np.array(testkeys).reshape(-1,1)

"""Step 4: Build a model

- KNN
"""

from sklearn.neighbors import KNeighborsClassifier
import time
import numpy as np
from sklearn.metrics import classification_report
t1=time.time()
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train,Y_train)
t2=time.time()
time_interval=t2-t1
devpre=neigh.predict(X_dev)#.reshape(1,-1).tolist()[0]
print(classification_report(Y_dev,devpre))
print("time interval for building model:"+str(time_interval*1000)+" ms")
t1=time.time()
testpre=neigh.predict(X_test)#.reshape(1,-1).tolist()[0]
t2=time.time()
time_interval=t2-t1
print("time interval for indexing data:"+str(time_interval*1000)+" ms")

"""- Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
import time
import numpy as np
from sklearn.metrics import classification_report
t1=time.time()
NB = GaussianNB()
NB.fit(X_train,Y_train)
t2=time.time()
time_interval=t2-t1
devpre=NB.predict(X_dev)#.reshape(1,-1).tolist()[0]
print(classification_report(Y_dev,devpre))
print("time interval for building model:"+str(time_interval*1000)+" ms")
t1=time.time()
testpre=NB.predict(X_test)#.reshape(1,-1).tolist()[0]
t2=time.time()
time_interval=t2-t1
print("time interval for indexing data:"+str(time_interval*1000)+" ms")

"""- Decision Tree"""

from sklearn import tree
import time
import numpy as np
from sklearn.metrics import classification_report
t1=time.time()
tree = tree.DecisionTreeClassifier()
tree.fit(X_train,Y_train)
t2=time.time()
time_interval=t2-t1
devpre=tree.predict(X_dev)#.reshape(1,-1).tolist()[0]
print(classification_report(Y_dev,devpre))
print("time interval for building model:"+str(time_interval*1000)+" ms")
t1=time.time()
testpre=tree.predict(X_test)#.reshape(1,-1).tolist()[0]
t2=time.time()
time_interval=t2-t1
print("time interval for indexing data:"+str(time_interval*1000)+" ms")

"""- Random Forest"""

from sklearn.ensemble import RandomForestClassifier
import time
import numpy as np
from sklearn.metrics import classification_report
t1=time.time()
forest = RandomForestClassifier()
forest.fit(X_train,Y_train)
t2=time.time()
time_interval=t2-t1
devpre=forest.predict(X_dev)#.reshape(1,-1).tolist()[0]
print(classification_report(Y_dev,devpre))
print("time interval for building model:"+str(time_interval*1000)+" ms")
t1=time.time()
testpre=forest.predict(X_test)#.reshape(1,-1).tolist()[0]
t2=time.time()
time_interval=t2-t1
print("time interval for indexing data:"+str(time_interval*1000)+" ms")

"""- DNN"""

from keras.utils.np_utils import to_categorical   
Y_train_NN = to_categorical(Y_train, num_classes=100)
Y_dev_NN = to_categorical(Y_dev, num_classes=100)

import tensorflow as tf
from tensorboard.plugins.hparams import api
from keras import models as md
from keras import layers as lr
import numpy as np
import time
t1=time.time()
model = md.Sequential()
model.add(lr.Dense(128,activation="relu"))
model.add(lr.Dense(128,activation="relu"))
# model.add(lr.Dense(32,activation="relu"))
model.add(lr.Dropout(0.2))
model.add(lr.Dense(100,activation="softmax"))
model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=["accuracy"])#compile the model
model.fit(X_train, Y_train_NN, epochs=16, batch_size=32)#fit the model
t2=time.time()
time_interval=t2-t1
loss, acc = model.evaluate(X_dev, Y_dev_NN)
print(loss,acc)
print("time interval for building model:"+str(time_interval*1000)+" ms")
t1=time.time()
model.predict(X_test)
t2=time.time()
time_interval=t2-t1
print("time interval for indexing data:"+str(time_interval*1000)+" ms")

Y_test=np.argmax(model.predict(X_test), axis=-1)
print(Y_test)